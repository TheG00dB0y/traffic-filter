{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traffic_filter_new.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE3J9hXAUf_X",
        "colab_type": "text"
      },
      "source": [
        "#Widerface data preprocessing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW2Dkqxda6O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from IPython.display import Image\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "from google.colab.patches import cv2_imshow\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZADKgsEyV2-d",
        "colab_type": "text"
      },
      "source": [
        "Copy widerface dataset (if not downloaded :\n",
        "[Dataset download page](http://shuoyang1213.me/WIDERFACE/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4IJVxKPYYYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/Train_images/WIDER_test.zip\" \"/content/wider_test.zip\" \n",
        "!unzip wider_test.zip\n",
        "!rm wider_test.zip -r\n",
        "\n",
        "!cp \"/content/drive/My Drive/Train_images/WIDER_train.zip\" \"/content/wider_train.zip\" \n",
        "!unzip wider_train.zip\n",
        "!rm wider_train.zip -r\n",
        "\n",
        "!cp \"/content/drive/My Drive/Train_images/WIDER_val.zip\" \"/content/wider_val.zip\" \n",
        "!unzip wider_val.zip\n",
        "!rm wider_val.zip -r\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDCptaWCUeHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download annotations\n",
        "!curl -L \"http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip\" > annotations.zip; unzip annotations.zip; rm annotations.zip\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOqZxaZZZQL1",
        "colab_type": "text"
      },
      "source": [
        "Set up empty folder structure \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJi1TDVgIyZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('train'):\n",
        "    os.makedirs('train')\n",
        "if not os.path.exists('train/images'):\n",
        "    os.makedirs('train/images')\n",
        "    os.makedirs('train/labels')\n",
        "if not os.path.exists('valid'):\n",
        "    os.makedirs('valid')\n",
        "if not os.path.exists('valid/images'):\n",
        "    os.makedirs('valid/images')\n",
        "    os.makedirs('valid/labels')\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsU6lljxSCxD",
        "colab_type": "text"
      },
      "source": [
        "Adding files to colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxcisVQXVF8f",
        "colab_type": "text"
      },
      "source": [
        "**train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EErj7s2udegn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_source_dir = \"/content/WIDER_train/images\"\n",
        "lines = open(\"/content/wider_face_split/wider_face_train_bbx_gt.txt\").readlines()\n",
        "img_output_dir = \"/content/train/images\"\n",
        "label_output_dir = \"/content/train/labels\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BY2HdYJPjhwS",
        "colab": {}
      },
      "source": [
        "for line in lines:\n",
        "  if line.endswith(\".jpg\\n\"):\n",
        "    box = []\n",
        "    line = line.strip()\n",
        "    line_list = line.split('/',1)\n",
        "    img_name = line_list[1].strip()\n",
        "    img_name_label = img_name.split(\".jpg\")[0]\n",
        "    try:\n",
        "      img = cv2.imread(os.path.join(img_source_dir,line))\n",
        "      img_h, img_w , _ = img.shape\n",
        "      copyfile(os.path.join(img_source_dir,line),os.path.join(img_output_dir,img_name))\n",
        "    except:\n",
        "      print(\"ERROR\")\n",
        "      continue\n",
        "\n",
        "  elif(len(line)>10):\n",
        "      line = line.split()\n",
        "      # print(line)\n",
        "      # print(os.path.join('train/labels',img_name_label + '.txt'))\n",
        "      # label_file = open(os.path.join('train/labels',img_name_label + '.txt'),\"w\") \n",
        "      #convert labels\n",
        "      [x1,y1,w,h] = line[:4]\n",
        "      # print(x1,y1,w,h)\n",
        "      x = (int(x1)  + int(w)/2 ) /img_w\n",
        "      y = (int(y1)  + int(h)/2 ) /img_h\n",
        "      w = int(w) / img_w\n",
        "      h = int(h) / img_h\n",
        "      if x or y or w or h == 0:\n",
        "        continue\n",
        "      # print(x,y,w,h)\n",
        "      label_line = '11 ' + str(x) + ' ' + str(y) + ' ' + str(w) + ' ' + str(h) + '\\n'\n",
        "      # print(label_line)\n",
        "      with open(os.path.join(label_output_dir,img_name_label + '.txt'),\"a\") as label_file:\n",
        "        label_file.write(label_line)\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKva5sbzTEeP",
        "colab_type": "text"
      },
      "source": [
        "**Test Set (added with train set)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvZ-hhi1juYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_source_dir = \"/content/WIDER_test/images\"\n",
        "lines = open(\"/content/wider_face_split/wider_face_test_filelist.txt\").readlines()\n",
        "img_output_dir = \"/content/train/images\"\n",
        "label_output_dir = \"/content/train/labels\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SlCsED3Ej0gc",
        "colab": {}
      },
      "source": [
        "for line in lines:\n",
        "  if line.endswith(\".jpg\\n\"):\n",
        "    box = []\n",
        "    line = line.strip()\n",
        "    line_list = line.split('/',1)\n",
        "    img_name = line_list[1].strip()\n",
        "    img_name_label = img_name.split(\".jpg\")[0]\n",
        "    try:\n",
        "      img = cv2.imread(os.path.join(img_source_dir,line))\n",
        "      img_h, img_w , _ = img.shape\n",
        "      copyfile(os.path.join(img_source_dir,line),os.path.join(img_output_dir,img_name))\n",
        "    except:\n",
        "      print(\"ERROR\")\n",
        "      continue\n",
        "\n",
        "  elif(len(line)>10):\n",
        "      line = line.split()\n",
        "      # print(line)\n",
        "      # print(os.path.join('train/labels',img_name_label + '.txt'))\n",
        "      # label_file = open(os.path.join('train/labels',img_name_label + '.txt'),\"w\") \n",
        "      #convert labels\n",
        "      [x1,y1,w,h] = line[:4]\n",
        "      # print(x1,y1,w,h)\n",
        "      x = (int(x1)  + int(w)/2 ) /img_w\n",
        "      y = (int(y1)  + int(h)/2 ) /img_h\n",
        "      w = int(w) / img_w\n",
        "      h = int(h) / img_h\n",
        "      if x or y or w or h == 0:\n",
        "        continue\n",
        "      # print(x,y,w,h)\n",
        "      label_line = '11 ' + str(x) + ' ' + str(y) + ' ' + str(w) + ' ' + str(h) + '\\n'\n",
        "      # print(label_line)\n",
        "      with open(os.path.join(label_output_dir,img_name_label + '.txt'),\"a\") as label_file:\n",
        "        label_file.write(label_line)\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbMmG4VTTw1x",
        "colab_type": "text"
      },
      "source": [
        "**Validation Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC-iS6lmTwc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_source_dir = \"/content/WIDER_val/images\"\n",
        "lines = open(\"/content/wider_face_split/wider_face_val_bbx_gt.txt\").readlines()\n",
        "img_output_dir = \"/content/valid/images\"\n",
        "label_output_dir = \"/content/valid/labels\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r--zkPXGso4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in lines:\n",
        "  if line.endswith(\".jpg\\n\"):\n",
        "    box = []\n",
        "    line = line.strip()\n",
        "    line_list = line.split('/',1)\n",
        "    img_name = line_list[1].strip()\n",
        "    img_name_label = img_name.split(\".jpg\")[0]\n",
        "    try:\n",
        "      img = cv2.imread(os.path.join(img_source_dir,line))\n",
        "      img_h, img_w , _ = img.shape\n",
        "      copyfile(os.path.join(img_source_dir,line),os.path.join(img_output_dir,img_name))\n",
        "    except:\n",
        "      print(\"ERROR\")\n",
        "      continue\n",
        "\n",
        "  elif(len(line)>10):\n",
        "      line = line.split()\n",
        "      # print(line)\n",
        "      # print(os.path.join('train/labels',img_name_label + '.txt'))\n",
        "      # label_file = open(os.path.join('train/labels',img_name_label + '.txt'),\"w\") \n",
        "      #convert labels\n",
        "      [x1,y1,w,h] = line[:4]\n",
        "      # print(x1,y1,w,h)\n",
        "      x = (int(x1)  + int(w)/2 ) /img_w\n",
        "      y = (int(y1)  + int(h)/2 ) /img_h\n",
        "      w = int(w) / img_w\n",
        "      h = int(h) / img_h\n",
        "      if x or y or w or h == 0:\n",
        "        continue\n",
        "      # print(x,y,w,h)\n",
        "      label_line = '11 ' + str(x) + ' ' + str(y) + ' ' + str(w) + ' ' + str(h) + '\\n'\n",
        "      # print(label_line)\n",
        "      with open(os.path.join(label_output_dir,img_name_label + '.txt'),\"a\") as label_file:\n",
        "        label_file.write(label_line)\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFtQR1ppgdja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm WIDER_test -r\n",
        "!rm WIDER_train -r\n",
        "!rm WIDER_val -r\n",
        "!rm wider_face_split -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a9AEZxGYciT",
        "colab_type": "text"
      },
      "source": [
        "Display image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS1jalg6bXxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # !ls \"/content/train/images\"\n",
        "# from IPython.display import Image\n",
        "# Image('/content/train/images/9_Press_Conference_Press_Conference_9_38.jpg')\n",
        "# # cv2_imshow('/content/train/images/9_Press_Conference_Press_Conference_9_38.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pXgk9mwZi3Q",
        "colab_type": "text"
      },
      "source": [
        "#Dataset for udacity self driving cars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wkiLfUFIraN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd \"/content/drive/My Drive/traffic filter\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JVvL73McM5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c8d3d73-3715-452b-8747-473ccbe30455"
      },
      "source": [
        "# !git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "!cp \"/content/drive/My Drive/traffic filter/yolov5\" \"/content/yolov5\" -r\n",
        "%cd yolov5\n",
        "#temporary until bug is fixed in current master\n",
        "# !git reset --hard 5ba1de0cdcc414c69ceb7a4c45eb1e3895eca32a\n",
        "%cd ..\n",
        "\n",
        "!pip install -r yolov5/requirements.txt  # install dependencies\n",
        "%cd yolov5\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from utils.google_utils import gdrive_download  # to download models/datasets\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.5.1+cu101 CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_dWxyEecUSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content/yolov5\n",
        "# !bash \"/content/yolov5/weights/download_weights.sh\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrpL0fFqcXH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49d65e29-e57f-4084-b750-705a8ee4a2b6"
      },
      "source": [
        "%cd \"/content\"\n",
        "!curl -L \"https://public.roboflow.ai/ds/XCnZS0hI6C?key=H6G2RBhLy3\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "clear_output()\n",
        "print(\"Dataset downloaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do01D4nQalez",
        "colab_type": "text"
      },
      "source": [
        "**Add new classes in the config (Change nc and add new class name 'face'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCa6nwxqzm1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm \"/content/data.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWNVfuJ9yt-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp  \"/content/drive/My Drive/data.yaml\" \"/content/data.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8pFXmC4ceOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "fc775f5e-f9b8-4048-abd4-c5cf2af54b51"
      },
      "source": [
        "%cat data.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: ../train/images\n",
            "val: ../valid/images\n",
            "\n",
            "nc: 12\n",
            "names: ['biker', 'car', 'pedestrian', 'trafficLight', 'trafficLight-Green', 'trafficLight-GreenLeft', 'trafficLight-Red', 'trafficLight-RedLeft', 'trafficLight-Yellow', 'trafficLight-YellowLeft', 'truck','face']"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6b2hoWAclvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        " \n",
        "img_source_dir = \"/content/export/images\"\n",
        "label_source_dir = \"/content/export/labels\"\n",
        "train_size = 0.8\n",
        "if not (isinstance(img_source_dir, str)):\n",
        "    raise AttributeError('img_source_dir must be a string')\n",
        "    \n",
        "if not os.path.exists(img_source_dir):\n",
        "    raise OSError('img_source_dir does not exist')\n",
        "    \n",
        "if not (isinstance(train_size, float)):\n",
        "    raise AttributeError('train_size must be a float')\n",
        "        \n",
        "    # Set up empty folder structure if not exists\n",
        "if not os.path.exists('train'):\n",
        "    os.makedirs('train')\n",
        "if not os.path.exists('train/images'):\n",
        "    os.makedirs('train/images')\n",
        "    os.makedirs('train/labels')\n",
        "if not os.path.exists('valid'):\n",
        "    os.makedirs('valid')\n",
        "if not os.path.exists('valid/images'):\n",
        "    os.makedirs('valid/images')\n",
        "    os.makedirs('valid/labels')\n",
        "            \n",
        "\n",
        "train_counter = 0\n",
        "validation_counter = 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1euUazzcoLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ba977852-f005-4260-9644-15a8cec2c05c"
      },
      "source": [
        "import cv2\n",
        "# Randomly assign an image to train or validation folder\n",
        "for filename in os.listdir(img_source_dir):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        fileparts = filename.split('.jpg')\n",
        "        # print(os.path.join(label_source_dir,fileparts[0] + '.txt'))\n",
        "        # print(os.path.join('train/images',filename))\n",
        "        # image = cv2.imread(os.path.join(img_source_dir,filename)) \n",
        "        # image = cv2.resize(image, (412, 412)) \n",
        "        if random.uniform(0, 1) <= train_size:\n",
        "\n",
        "            # copyfile(image, os.path.join('train/images',filename))\n",
        "            copyfile(os.path.join(img_source_dir,filename), os.path.join('/content/train/images',filename))\n",
        "            copyfile(os.path.join(label_source_dir,fileparts[0] + '.txt'), os.path.join('/content/train/labels',fileparts[0] + '.txt'))\n",
        "            train_counter += 1\n",
        "        else:\n",
        "            copyfile(os.path.join(img_source_dir,filename), os.path.join('/content/valid/images',filename))\n",
        "            copyfile(os.path.join(label_source_dir,fileparts[0] + '.txt'), os.path.join('/content/valid/labels',fileparts[0] + '.txt'))\n",
        "            validation_counter += 1 \n",
        "            \n",
        "print('Copied ' + str(train_counter) + ' images to train' )\n",
        "print('Copied ' + str(validation_counter) + ' images to valid' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copied 11951 images to train\n",
            "Copied 3049 images to valid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmKStzNlcv9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm export -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMvFXT0NcI2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3399d99f-fd7b-4489-8419-c9474536e2f3"
      },
      "source": [
        "#Files Count\n",
        "path, dirs, files = next(os.walk(\"/content/train/images\"))\n",
        "file_count = len(files)\n",
        "print(file_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYH_YPQsNKjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcmRlEPUdQrV",
        "colab_type": "text"
      },
      "source": [
        "# Define Model Configuration and Architecture\n",
        "\n",
        "---script that defines the parameters for our model like the number of classes, anchors, and each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYcF5tvNdOlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define number of classes based on YAML\n",
        "import yaml\n",
        "with open(\"data.yaml\", 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FfTQ3pZdVg6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e5c3ccf1-0be6-48de-aec9-09d8433b6674"
      },
      "source": [
        "%cd \"/content\"\n",
        "##write custom model .yaml\n",
        "#you can configure this based on other YOLOv5 models in the models directory\n",
        "with open('yolov5/models/custom_yolov5s.yaml', 'w') as f:\n",
        "  # parameters\n",
        "  f.write('nc: ' + num_classes + '\\n')\n",
        "  #f.write('nc: ' + str(len(class_labels)) + '\\n')\n",
        "  f.write('depth_multiple: 0.33'  + '\\n') # model depth multiple\n",
        "  f.write('width_multiple: 0.50'  + '\\n')  # layer channel multiple\n",
        "  f.write('\\n')\n",
        "  f.write('anchors:' + '\\n')\n",
        "  f.write('  - [10,13, 16,30, 33,23] ' + '\\n')\n",
        "  f.write('  - [30,61, 62,45, 59,119]' + '\\n')\n",
        "  f.write('  - [116,90, 156,198, 373,326] ' + '\\n')\n",
        "  f.write('\\n')\n",
        "\n",
        "  f.write('backbone:' + '\\n')\n",
        "  f.write('  [[-1, 1, Focus, [64, 3]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [128, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 3, Bottleneck, [128]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [256, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 9, BottleneckCSP, [256]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [512, 3, 2]], ' + '\\n')\n",
        "  f.write('   [-1, 9, BottleneckCSP, [512]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [1024, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 1, SPP, [1024, [5, 9, 13]]],' + '\\n')\n",
        "  f.write('   [-1, 6, BottleneckCSP, [1024]],' + '\\n')\n",
        "  f.write('  ]' + '\\n')\n",
        "  f.write('\\n')\n",
        "\n",
        "  f.write('head:'  + '\\n')\n",
        "  f.write('  [[-1, 3, BottleneckCSP, [1024, False]],'  + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  f.write('   [-2, 1, nn.Upsample, [None, 2, \"nearest\"]],' + '\\n')\n",
        "  \n",
        "  f.write('   [[-1, 6], 1, Concat, [1]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [512, 1, 1]],' + '\\n')\n",
        "  f.write('   [-1, 3, BottleneckCSP, [512, False]],' + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  \n",
        "  f.write('   [-2, 1, nn.Upsample, [None, 2, \"nearest\"]],' + '\\n')\n",
        "  f.write('   [[-1, 4], 1, Concat, [1]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [256, 1, 1]],' + '\\n')\n",
        "  f.write('   [-1, 3, BottleneckCSP, [256, False]],' + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  f.write('\\n' )\n",
        "  f.write('   [[], 1, Detect, [nc, anchors]],' + '\\n')\n",
        "  f.write('  ]' + '\\n')\n",
        "\n",
        "print('custom model config written!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "custom model config written!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP5lbGordlft",
        "colab_type": "text"
      },
      "source": [
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** set the path to our yaml file\n",
        "- **cfg:** specify our model configuration\n",
        "- **weights:** specify a custom path to weights. \n",
        "- **name:** result names\n",
        "- **nosave:** only save the final checkpoint\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN4Gen6Zdn0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30e18390-dad2-4f7d-f148-96ba5aaa5118"
      },
      "source": [
        "# train yolov5s on custom data for 50 epochs\n",
        "# time its performance\n",
        "%%time\n",
        "%cd \"/content/yolov5\"\n",
        "!python train.py --img 512 --batch 16 --epochs 20 --data \"/content/drive/My Drive/data.yaml\" --cfg \"models/custom_yolov5s.yaml\" --weights \"/content/drive/My Drive/trained_weight/combined_best_3.pt\" --name yolo_results "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex\n",
            "{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n",
            "Your branch is behind 'origin/master' by 210 commits, and can be fast-forwarded.\n",
            "  (use \"git pull\" to update your local branch)\n",
            "\n",
            "Namespace(adam=False, batch_size=16, bucket='', cache_images=False, cfg='models/custom_yolov5s.yaml', data='/content/drive/My Drive/data.yaml', device='', epochs=20, evolve=False, img_size=[512], multi_scale=False, name='yolo_results', noautoanchor=False, nosave=False, notest=False, rect=False, resume=False, single_cls=False, weights='/content/drive/My Drive/trained_weight/combined_best_3.pt')\n",
            "Using CPU\n",
            "\n",
            "2020-07-29 08:17:15.768733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     20672  models.common.Bottleneck                [64, 64]                      \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1905152  models.common.BottleneckCSP             [512, 512, 2]                 \n",
            " 10                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 11                -1  1     26163  torch.nn.modules.conv.Conv2d            [512, 51, 1, 1, 0]            \n",
            " 12                -2  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 13           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 14                -1  1    197120  models.common.Conv                      [768, 256, 1, 1]              \n",
            " 15                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 16                -1  1     13107  torch.nn.modules.conv.Conv2d            [256, 51, 1, 1, 0]            \n",
            " 17                -2  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1     49408  models.common.Conv                      [384, 128, 1, 1]              \n",
            " 20                -1  1     78720  models.common.BottleneckCSP             [128, 128, 1, False]          \n",
            " 21                -1  1      6579  torch.nn.modules.conv.Conv2d            [128, 51, 1, 1, 0]            \n",
            " 22      [-1, 16, 11]  1         0  models.yolo.Detect                      [12, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]]\n",
            "Model Summary: 165 layers, 6.89078e+06 parameters, 6.89078e+06 gradients\n",
            "\n",
            "Optimizer groups: 54 .bias, 60 conv.weight, 51 other\n",
            "Reading image shapes: 100% 40928/40928 [00:19<00:00, 2074.14it/s]\n",
            "Caching labels ../train/labels (10557 found, 28975 missing, 1394 empty, 2 duplicate, for 40928 images): 100% 40928/40928 [00:01<00:00, 21290.28it/s]\n",
            "Saving labels to ../train/labels.npy for faster future loading\n",
            "Reading image shapes: 100% 6275/6275 [00:00<00:00, 12059.94it/s]\n",
            "Caching labels ../valid/labels (2682 found, 2457 missing, 367 empty, 1 duplicate, for 6275 images): 100% 6275/6275 [00:00<00:00, 16069.36it/s]\n",
            "Saving labels to ../valid/labels.npy for faster future loading\n",
            "\n",
            "Analyzing anchors... Best Possible Recall (BPR) = 0.9995\n",
            "Image sizes 512 train, 512 test\n",
            "Using 2 dataloader workers\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      9/19        0G   0.05101   0.01141  0.009071   0.07149        20       512:   2% 56/2558 [14:32<10:44:28, 15.45s/it]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFulYuqFd0Hh",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov5s_results`. (If given no name, it defaults to `results.txt`.) The results file is plotted as a png after training completes.\n",
        " Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OXldmBbdzth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAOyDSYPd8fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can also output some older school graphs if the tensor board isn't working for whatever reason... \n",
        "from utils.utils import plot_results; plot_results()  # plot results.txt as results.png\n",
        "Image(filename='./results.png', width=1000)  # view results.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6--xl0PQd-9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/yolov5/weights/best_merged_yolo_results.pt\" \"/content/drive/My Drive/trained_weight/combined_best_4.pt\"\n",
        "!cp \"/content/yolov5/weights/last_merged_yolo_results.pt\"  \"/content/drive/My Drive/trained_weight/combined_last_4.pt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaRIcsXzpvvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "971edaf2-6e2d-4eb6-c6a3-ca822a1cf337"
      },
      "source": [
        "!cp \"/content/yolov5/weights/best.pt\" \"/content/drive/My Drive/trained_weight/combined_best_checkpoint_4.pt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/yolov5/weights/best.pt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzyab7GEYQaU",
        "colab_type": "text"
      },
      "source": [
        "# Image testing (Not part of the code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6DtieP4gODW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "image = cv2.imread(\"/content/1478019952686311006_jpg.rf.a8dc9db36ae4a7e6ecf234f85885a153.jpg\")\n",
        "start_point = (int(0.928- (0.0364/2)), int(0.510 - (0.218/2)))\n",
        "  \n",
        "# Ending coordinate, here (220, 220) \n",
        "# represents the bottom right corner of rectangle \n",
        "end_point = (int(0.928 + (0.0364/2)), int(0.510 + (0.218/2)))\n",
        "  \n",
        "# Blue color in BGR \n",
        "color = (255, 0, 0) \n",
        "  \n",
        "# Line thickness of 2 px \n",
        "thickness = 100\n",
        "  \n",
        "# Using cv2.rectangle() method \n",
        "# Draw a rectangle with blue line borders of thickness of 2 px \n",
        "# image = cv2.rectangle(image, start_point, end_point, color, thickness) \n",
        "print(image.shape)\n",
        "# 0.9286458333333333 * 512\n",
        "image = cv2.circle(image, (int(0.9286458333333333 * 512),int(0.5108333333333334 * 512)), radius=0, color=(0, 0, 255), thickness=2)\n",
        "cv2_imshow(image)\n",
        "344 347 84 138 0 0 0 0 0 0 \n",
        "539 353 90 117 1 0 0 0 0 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdxGqVTDga4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "image = cv2.imread(\"/content/4_Dancing_Dancing_4_49.jpg\")\n",
        "\n",
        "start_point = (344,347)\n",
        "end_point = (344 + 84,347 +138)\n",
        "color = (255, 0, 0) \n",
        "thickness = 2\n",
        "image = cv2.circle(image, (167,301), radius=0, color=(0, 0, 255), thickness=10)\n",
        "\n",
        "image = cv2.rectangle(image, start_point, end_point, color, thickness) \n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnltQ-n9ppQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm \"/content/wider_face_split\" -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adgCYDiYR4Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/train\" \"/content/drive/My Drive/traffic filter/train\" -r\n",
        "!cp \"/content/valid\" \"/content/drive/My Drive/traffic filter/valid\" -r\n",
        "!cp \"/content/yolov5\" \"/content/drive/My Drive/traffic filter/yolov5\" -r\n",
        "!cp \"/content/data.yaml\" \"/content/drive/My Drive/traffic filter/data.yaml\" -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZOORVkaSuFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVw6rwE8lxRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}